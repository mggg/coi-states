{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import contextily as ctx\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from submission_analysis.crosswalk import Crosswalk\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "db_path = '../../WI/data/wi_cluster_db_20210820.pkl'\n",
    "clusters_path = None\n",
    "block_2010_to_block_2020_crosswalk_path = '../../WI/data/tab2010_tab2020_st55_wi.txt'\n",
    "block_2020_shp_path = '../../WI/data/tl_2020_55_tabblock20'\n",
    "base_shp_path = '../data/tl_2020_us_county'\n",
    "num_clusters = 40\n",
    "state_fips_code = '55'\n",
    "cluster_name_prefix = 'A'  # Moon's versioning scheme\n",
    "cluster_cores = True\n",
    "cluster_core_threshold = 3  # minimum number of times a block must appear in a cluster to be considered \"core\"\n",
    "output_dir = '../../WI/outputs'\n",
    "output_prefix = 'WI_20210822_geo32'\n",
    "crs = 'EPSG:32616'\n",
    "output_formats = ['tex', 'png_summary']\n",
    "output_columns = ['districtr_id', 'portal_url', 'submission_title', 'submission_text', 'area_name', 'area_text', 'cluster']\n",
    "excluded_submissions = {}\n",
    "reassigned_submissions = {}\n",
    "portal_url_prefix = 'https://portal.wisconsin-mapping.org/submission/'\n",
    "swap_area_columns = False\n",
    "\n",
    "# choose the largest connected component (by population) of each COI when generating PNGs/shapefiles\n",
    "force_connected = True\n",
    "block_dual_graph_path = '../../WI/data/tl_2020_55_tabblock20.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geo_enabled = any(fmt in output_formats for fmt in ('shapefile', 'png', 'png_summary', 'csv_geo'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zk8P6Ejvp11"
   },
   "outputs": [],
   "source": [
    "if db_path:\n",
    "  db = pickle.load(open(db_path, 'rb'))\n",
    "  clusters = db.clusters_from_number(num_clusters)\n",
    "elif clusters_path:\n",
    "  clusters = pd.read_csv(clusters_path)\n",
    "for col in ('block_groups_2010', 'labels'): \n",
    "  try:\n",
    "    clusters[col] = clusters[col].apply(literal_eval) \n",
    "  except ValueError:\n",
    "    pass  # doesn't need to be parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = Crosswalk(block_2010_to_block_2020_crosswalk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: connected component filtering.\n",
    "if force_connected:\n",
    "  graph = nx.readwrite.json_graph.adjacency_graph(json.load(open(block_dual_graph_path)))\n",
    "  graph = nx.relabel_nodes(graph, mapping=dict(graph.nodes('GEOID20')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if geo_enabled:\n",
    "  blocks_2020_gdf = gpd.read_file(block_2020_shp_path).set_index('GEOID20').to_crs(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_gdf = gpd.read_file(base_shp_path).to_crs(crs)\n",
    "if 'STATEFP' in base_gdf.columns:\n",
    "  base_gdf = base_gdf[base_gdf['STATEFP'] == state_fips_code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Cluster surgery (filtering) ✂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['clusters'] = clusters.apply(\n",
    "  lambda row: reassigned_submissions.get(str(row.name), row['clusters']),\n",
    "  axis=1\n",
    ")\n",
    "clusters = clusters[~clusters.index.isin(excluded_submissions)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 block frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = {}\n",
    "for cluster_id in tqdm(clusters['clusters'].unique()):\n",
    "  cluster_df = clusters[clusters['clusters'] == cluster_id]\n",
    "  \n",
    "  bg_2010_count = Counter()\n",
    "  for bgs in cluster_df['block_groups_2010']:\n",
    "    for bg in bgs:\n",
    "      bg_2010_count[bg] += 1\n",
    "  \n",
    "  block_2020_count = Counter()\n",
    "  for bg, count in bg_2010_count.items():\n",
    "    if count > 0:\n",
    "      for block_2020 in cw.map_2010_block_groups([bg]):\n",
    "        block_2020_count[block_2020] += count\n",
    "  cluster_counts[cluster_id] = block_2020_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ More cluster surgery (typing) ✂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters.rename(columns={'clusters': 'cluster'})\n",
    "if swap_area_columns:\n",
    "  # Correct for an error in the 8/9 databases.\n",
    "  clusters = clusters.rename(columns={\n",
    "    'area_text': 'area_name',\n",
    "    'area_name': 'area_text'\n",
    "  })\n",
    "\n",
    "for text_col in ('area_name', 'area_text', 'submission_text', 'cluster'):\n",
    "  clusters[text_col] = clusters[text_col].astype(str)\n",
    "  \n",
    "clusters.loc[clusters['area_name'] == 'nan', 'area_name'] = ''\n",
    "clusters.loc[clusters['area_text'] == 'nan', 'area_text'] = ''\n",
    "clusters.loc[clusters['submission_text'] == 'nan', 'submission_text'] = ''\n",
    "clusters.loc[clusters['submission_text'] == '0', 'submission_text'] = ''\n",
    "clusters['cluster'] = cluster_name_prefix + clusters['cluster']\n",
    "clusters['labels'] = clusters['labels'].apply(lambda s: ', '.join(s))\n",
    "clusters.index.name = 'plan_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output formats (per cluster)\n",
    "\n",
    "* `shapefile` – Shapefile containing the subset of 2020 blocks within the cluster, with `count` and `freq` attributes.\n",
    "* `csv_geo` - List of 2020 blocks within the cluster, with `count` and `freq` attributes.\n",
    "* `csv_comment` – Table containing COI submissions (including labels) supporting the cluster.\n",
    "* `html` - Table containing COI submissions (including labels) supporting the cluster.\n",
    "* `tex` - Table containing COI submissions (without labels) supporting the cluster.\n",
    "* `png` - Block-level heatmap (based on `count`) of the cluster with the state's counties as the basemap.\n",
    "\n",
    "### Attributes\n",
    "* `count` - The number of supporting COIs a block appears in.\n",
    "* `freq` - `count`, but normalized (0-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output_dir = os.path.join(output_dir, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ext in output_formats:\n",
    "  os.makedirs(os.path.join(full_output_dir, ext), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output_columns = [\n",
    "  col for col in output_columns\n",
    "  if col in clusters.columns or col == 'portal_url'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\LaTeX$ formatting\n",
    "Before generating a $\\LaTeX$ table from a submissions DataFrame, we fuse columns:\n",
    "* The portal ID (index) and Districtr ID (`districtr_id`) are fused into the `Portal Link (Districtr)` column; the raw portal ID is replaced with link to the appropriate mapping portal if a portal URL prefix is available.\n",
    "* `submission_title` and `submission_text` are fused into the `Overall Submission Information` column. The title is **bolded** with `\\textbf{}`.\n",
    "* `area_name` and `area_text` are formatted similarly and fused into the `Individual Area Information` column. `\n",
    "* Labels, which are for internal use only, are removed.\n",
    "\n",
    "We use Pandas' `.to_latex()` to generate an initial $\\LaTeX$ table from this fused DataFrame; all columns are truncated at 2000 characters (approximately 7.143 tweets). We then apply some styling modifications:\n",
    "* We use `supertabular` instead of `tabular` to enable stretching table entries across pages.\n",
    "* We [use `arraystretch` to increase the table's vertical padding](https://tex.stackexchange.com/a/31704).\n",
    "* We fix column widths:\n",
    "  * `Portal Link Districtr` – .48in\n",
    "  * `Overall Submission Information` - 3.5in\n",
    "  *`Individual Area Information` - 2in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tex_joint_columns(row, columns=('area_name', 'area_text')):\n",
    "  \"\"\"Fuses two columns (name + text) into a single column with conditional formatting.\"\"\"\n",
    "  name = row[columns[0]].strip()\n",
    "  text = row[columns[1]].strip()\n",
    "  if name and text:\n",
    "    return '\\\\textbf{' + name + ':} ' + text\n",
    "  elif name and not text:\n",
    "    return '\\\\textbf{' + name + '.}'\n",
    "  return text\n",
    "\n",
    "\n",
    "def format_tex(submissions_df, portal_url_prefix=None, max_colwidth_chars=2000):\n",
    "  \"\"\"Generates LaTeX submission tables according to MGGG report specs.\"\"\"\n",
    "  submissions_tex = submissions_df.copy()\n",
    "  submissions_tex['portal_id'] = submissions_tex.index.str.split('-').str[0]\n",
    "  submissions_tex['part_id'] = submissions_tex.index.str.split('-').str[1]\n",
    "  submissions_tex['plan_link'] = submissions_tex['portal_id']\n",
    "  submissions_tex = submissions_tex.set_index(['portal_id', 'part_id']).sort_index(level=1).reset_index()\n",
    "  \n",
    "  if portal_url_prefix:\n",
    "    submissions_tex['plan_link'] = submissions_tex['plan_link'].apply(\n",
    "      lambda portal_id: '\\href{' + portal_url_prefix + portal_id.split('-')[0] + '}{' + portal_id + '}'\n",
    "    )\n",
    "  submissions_tex['districtr_id'] = submissions_tex['districtr_id'].str.split('-').str[0].str.strip()\n",
    "  submissions_tex['Portal Link (Districtr)'] = submissions_tex['plan_link'] + ' (' + submissions_tex['districtr_id'] + ')'\n",
    "  submissions_tex['Individual Area Information'] = submissions_tex.apply(format_tex_joint_columns, axis=1)\n",
    "  submissions_tex['Overall Submission Information'] = submissions_tex.apply(\n",
    "    lambda row: format_tex_joint_columns(row, ('submission_title', 'submission_text')),\n",
    "    axis=1\n",
    "  )\n",
    "  display_cols = ['Portal Link (Districtr)', 'Overall Submission Information', 'Individual Area Information']\n",
    "  # oof. (see https://stackoverflow.com/a/46974532)\n",
    "  submissions_tex = submissions_tex[display_cols].set_index(display_cols)\n",
    "  with pd.option_context('max_colwidth', max_colwidth_chars):\n",
    "    tex = submissions_tex.to_latex(index=True, multirow=True)\n",
    "  subs = {\n",
    "    '\\\\textbackslash href\\{': '\\href{',\n",
    "    '\\}\\{': '}{',\n",
    "    '\\} (': '} (',\n",
    "    '\\\\textbackslash n': ' ',  # suppress newlines\n",
    "    '\\\\textbackslash textbf\\{': '\\\\textbf{',\n",
    "    ':\\}': ':}',\n",
    "    '.\\}': '.}'\n",
    "  }\n",
    "  for start, end in subs.items():\n",
    "    tex = tex.replace(start, end)\n",
    "  tex_lines = tex.split('\\n')    \n",
    "  return '\\n'.join(tex_lines[5:-3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def largest_connected_block_component(gdf):\n",
    "  \"\"\"Finds the largest connected component of a block-level cluster.\"\"\"\n",
    "  # Fetching block-level 2020 populations is still annoying as of\n",
    "  # 2021-08-25---the Census API still only has 2000/2010 data---\n",
    "  # so we use block count as a rough proxy for population-weighted\n",
    "  # component size.\n",
    "  if gdf.empty:\n",
    "    return gdf\n",
    "  subgraph = nx.subgraph(graph, list(gdf.index))\n",
    "  components = nx.connected_components(subgraph)\n",
    "  largest_component = sorted(components, key=len)[-1]\n",
    "  component_ids = set(graph.nodes[b]['GEOID20'] for b in largest_component)\n",
    "  return gdf[gdf.index.to_series().isin(component_ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_dissolved = []\n",
    "cluster_cores_dissolved = []\n",
    "\n",
    "for cluster_short_id, counts in tqdm(cluster_counts.items()):\n",
    "  cluster_id = cluster_name_prefix + str(cluster_short_id)\n",
    "  cluster_label = f'{output_prefix}_cluster_{cluster_id}'\n",
    "\n",
    "  df = pd.DataFrame.from_dict(counts, orient='index', columns=['count'])\n",
    "  df.index.name = 'GEOID20'\n",
    "  df['freq'] = df['count'] / df['count'].max()\n",
    "  submissions = clusters[clusters['cluster'] == cluster_id].copy()\n",
    "  submissions['portal_url'] = portal_url_prefix + submissions.index.to_series().str.split('-').str[0]\n",
    "  \n",
    "  submissions = submissions[filtered_output_columns]\n",
    "  if 'csv_geo' in output_formats:\n",
    "    df.to_csv(f'{full_output_dir}/csv_geo/{cluster_label}.csv')\n",
    "    \n",
    "  if 'csv_comments' in output_formats:\n",
    "    submissions.to_csv(f'{full_output_dir}/csv_comments/{cluster_label}.csv')\n",
    "    \n",
    "  if 'html' in output_formats:\n",
    "    html = submissions.to_html()\n",
    "    html = html.replace('\\\\n', '<br>').replace('\\\\t', ' ')\n",
    "    with open(f'{full_output_dir}/html/{cluster_label}.html', 'w') as f:\n",
    "      if portal_url_prefix:\n",
    "        for portal_id in submissions.index:\n",
    "          url = portal_url_prefix + portal_id.split('-')[0]\n",
    "          html = html.replace(\n",
    "            portal_id,\n",
    "            f'<a href=\"{url}\" target=\"_blank\">{portal_id}</a>'\n",
    "          )\n",
    "      f.write(html)\n",
    "  \n",
    "  if 'tex' in output_formats:    \n",
    "      with open(f'{full_output_dir}/tex/{cluster_label}.tex', 'w') as f:\n",
    "        f.write(format_tex(submissions, portal_url_prefix))\n",
    "  \n",
    "  if geo_enabled:\n",
    "    gdf = gpd.GeoDataFrame(df).join(blocks_2020_gdf[['geometry']])\n",
    "    gdf.crs = crs\n",
    "    if force_connected:\n",
    "      gdf = largest_connected_block_component(gdf)\n",
    "    \n",
    "    if cluster_cores:\n",
    "      core_gdf = gdf[gdf['count'] >= cluster_core_threshold]\n",
    "      if force_connected:\n",
    "        core_gdf = largest_connected_block_component(core_gdf)\n",
    "    \n",
    "    if 'shapefile' in output_formats:\n",
    "      gdf.to_file(f'{full_output_dir}/shapefile/{cluster_label}')\n",
    "      \n",
    "    if 'png' in output_formats:\n",
    "      fig, ax = plt.subplots(figsize=(10, 8), dpi=100)\n",
    "      base_gdf.plot(ax=ax, edgecolor='black', linewidth=2)\n",
    "      base_gdf.plot(color='#fffff5', edgecolor='#e5e5e5', ax=ax)\n",
    "      gdf.plot(ax=ax, column='count', cmap='YlOrRd', #'viridis_r',\n",
    "                           linewidth=0, edgecolor='none', antialiased=False,\n",
    "                           vmin=0, vmax=10)\n",
    "      ax.axis('off')  \n",
    "      plt.savefig(f'{full_output_dir}/png/{cluster_id}.png',\n",
    "                  dpi=300, transparent=True, bbox_inches='tight')\n",
    "      plt.close()\n",
    "      \n",
    "    if 'png_summary' in output_formats:\n",
    "      dissolved = gdf.dissolve()\n",
    "      if not dissolved.empty:\n",
    "        clusters_dissolved.append({'cluster': cluster_id, 'geometry': dissolved.iloc[0].geometry})\n",
    "      if cluster_cores:\n",
    "        core_dissolved = core_gdf.dissolve()\n",
    "        if not core_dissolved.empty:\n",
    "          cluster_cores_dissolved.append({\n",
    "            'cluster': cluster_id,\n",
    "            'geometry': core_dissolved.iloc[0].geometry\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_summary(dissolved_gdf):\n",
    "  \"\"\"Plots dissolved clusters (or cluster cores).\"\"\"\n",
    "  # Contextily expects the Web Mercator projection.\n",
    "  dissolved_gdf = dissolved_gdf.to_crs(epsg=3857)\n",
    "  \n",
    "  # A (futile?) attempt to improve the z-order such that smaller clusters\n",
    "  # are more visible.\n",
    "  dissolved_gdf['area'] = dissolved_gdf.geometry.apply(lambda geom: geom.area)\n",
    "  dissolved_gdf = dissolved_gdf.sort_values(by=['area'], ascending=False)\n",
    "  \n",
    "  # (see https://jcutrer.com/python/learn-geopandas-plotting-usmaps)\n",
    "  ax = dissolved_gdf.plot(figsize=(20, 16), alpha=0.6, edgecolor='black', column='cluster')\n",
    "  dissolved_gdf.apply(\n",
    "    lambda x: ax.annotate(\n",
    "      text=x.cluster,\n",
    "      xy=x.geometry.centroid.coords[0],\n",
    "      ha='center',\n",
    "      fontsize=16,\n",
    "      fontname='Helvetica',\n",
    "      fontweight='bold'),\n",
    "    axis=1)\n",
    "  ctx.add_basemap(ax, source=ctx.providers.CartoDB.Voyager)\n",
    "  ax.axis('off')\n",
    "  return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'png_summary' in output_formats:\n",
    "  clusters_gdf = gpd.GeoDataFrame(clusters_dissolved)\n",
    "  clusters_gdf.crs = crs\n",
    "  plot_summary(clusters_gdf)\n",
    "  plt.savefig(f'{full_output_dir}/png_summary/{output_prefix}_{cluster_name_prefix}_summary.png', dpi=300)\n",
    "  plt.close()\n",
    "  \n",
    "  if cluster_cores:\n",
    "    cluster_cores_gdf = gpd.GeoDataFrame(cluster_cores_dissolved)\n",
    "    cluster_cores_gdf.crs = crs\n",
    "    plot_summary(cluster_cores_gdf)\n",
    "    plt.savefig(f'{full_output_dir}/png_summary/{output_prefix}_{cluster_name_prefix}_cores_summary.png', dpi=300)\n",
    "    plt.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "avg_hausdorff_clustering_for_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
