{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import geopandas as gpd\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from submission_analysis.crosswalk import Crosswalk\n",
    "from tqdm import tqdm, trange\n",
    "from ast import literal_eval\n",
    "from scipy.cluster import hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "db_path = '../../WI/data/wi_cluster_db_20210820.pkl'\n",
    "clusters_path = None\n",
    "block_2010_to_block_2020_crosswalk_path = '../../WI/data/tab2010_tab2020_st55_wi.txt'\n",
    "block_2020_shp_path = '../../WI/data/tl_2020_55_tabblock20'\n",
    "county_shp_path = '../data/tl_2020_us_county'\n",
    "num_clusters = 40\n",
    "state_fips_code = '55'\n",
    "cluster_name_prefix = 'A'  # Moon's versioning scheme\n",
    "output_dir = '../../WI/outputs'\n",
    "output_prefix = 'WI_20210822_geo32'\n",
    "crs = 'EPSG:32616'\n",
    "output_formats = ['tex']\n",
    "output_columns = ['id', 'districtr_id', 'submission_title', 'submission_text', 'area_name', 'area_text', 'labels']\n",
    "excluded_submissions = {}\n",
    "reassigned_submissions = {}\n",
    "portal_url_prefix = 'https://portal.wisconsin-mapping.org/submission/'\n",
    "swap_area_columns = False\n",
    "\n",
    "# choose the largest connected component (by population) of each COI when generating PNGs/shapefiles\n",
    "force_connected = False  \n",
    "block_dual_graph_path = '../../WI/data/tl_2020_55_tabblock20.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3zk8P6Ejvp11"
   },
   "outputs": [],
   "source": [
    "if db_path:\n",
    "  db = pickle.load(open(db_path, 'rb'))\n",
    "  clusters = db.clusters_from_number(num_clusters)\n",
    "elif clusters_path:\n",
    "  clusters = pd.read_csv(clusters_path)\n",
    "for col in ('block_groups_2010', 'labels'): \n",
    "  try:\n",
    "    clusters[col] = clusters[col].apply(literal_eval) \n",
    "  except ValueError:\n",
    "    pass  # doesn't need to be parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cw = Crosswalk(block_2010_to_block_2020_crosswalk_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: connected component filtering.\n",
    "if force_connected:\n",
    "  graph = nx.readwrite.json_graph.adjacency_graph(json.load(open(block_dual_graph_path)))\n",
    "  graph = nx.relabel_nodes(graph, mapping=dict(graph.nodes('GEOID20')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'shapefile' in output_formats or 'png' in output_formats:\n",
    "  blocks_2020_gdf = gpd.read_file(block_2020_shp_path).set_index('GEOID20').to_crs(crs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counties_gdf = gpd.read_file(county_shp_path).to_crs(crs)\n",
    "if 'STATEFP' in counties_gdf.columns:\n",
    "  counties_gdf = counties_gdf[counties_gdf['STATEFP'] == state_fips_code]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ Cluster surgery (filtering) ✂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters['clusters'] = clusters.apply(\n",
    "  lambda row: reassigned_submissions.get(str(row.name), row['clusters']),\n",
    "  axis=1\n",
    ")\n",
    "clusters = clusters[~clusters.index.isin(excluded_submissions)].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2020 block frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_counts = []\n",
    "for cluster_idx in trange(1, max(clusters['clusters']) + 1):     \n",
    "    cluster_df = clusters[clusters['clusters'] == cluster_idx]\n",
    "    bg_2010_count = Counter()\n",
    "    for bgs in cluster_df['block_groups_2010']:\n",
    "      for bg in bgs:\n",
    "        bg_2010_count[bg] += 1\n",
    "    block_2020_count = Counter()\n",
    "    for bg, count in bg_2010_count.items():\n",
    "      if count > 0:\n",
    "        for block_2020 in cw.map_2010_block_groups([bg]):\n",
    "          block_2020_count[block_2020] += count\n",
    "    cluster_counts.append(block_2020_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✂️ More cluster surgery (typing) ✂️"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters = clusters.rename(columns={'clusters': 'cluster'})\n",
    "if swap_area_columns:\n",
    "  # Correct for an error in the 8/9 databases.\n",
    "  clusters = clusters.rename(columns={\n",
    "    'area_text': 'area_name',\n",
    "    'area_name': 'area_text'\n",
    "  })\n",
    "\n",
    "for text_col in ('area_name', 'area_text', 'submission_text', 'cluster'):\n",
    "  clusters[text_col] = clusters[text_col].astype(str)\n",
    "  \n",
    "clusters.loc[clusters['area_name'] == 'nan', 'area_name'] = ''\n",
    "clusters.loc[clusters['area_text'] == 'nan', 'area_text'] = ''\n",
    "clusters.loc[clusters['submission_text'] == 'nan', 'submission_text'] = ''\n",
    "clusters.loc[clusters['submission_text'] == '0', 'submission_text'] = ''\n",
    "clusters['cluster'] = cluster_name_prefix + clusters['cluster']\n",
    "clusters['labels'] = clusters['labels'].apply(lambda s: ', '.join(s))\n",
    "clusters.index.name = 'plan_id'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output formats (per cluster)\n",
    "\n",
    "* `shapefile` – Shapefile containing the subset of 2020 blocks within the cluster, with `count` and `freq` attributes.\n",
    "* `csv_geo` - List of 2020 blocks within the cluster, with `count` and `freq` attributes.\n",
    "* `csv_comment` – Table containing COI submissions (including labels) supporting the cluster.\n",
    "* `html` - Table containing COI submissions (including labels) supporting the cluster.\n",
    "* `tex` - Table containing COI submissions (without labels) supporting the cluster.\n",
    "* `png` - Block-level heatmap (based on `count`) of the cluster with the state's counties as the basemap.\n",
    "\n",
    "### Attributes\n",
    "* `count` - The number of supporting COIs a block appears in.\n",
    "* `freq` - `count`, but normalized (0-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_output_dir = os.path.join(output_dir, output_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ext in output_formats:\n",
    "  os.makedirs(os.path.join(full_output_dir, ext), exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_output_columns = [\n",
    "  col for col in output_columns\n",
    "  if col in clusters.columns\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\LaTeX$ formatting\n",
    "Before generating a $\\LaTeX$ table from a submissions DataFrame, we fuse columns:\n",
    "* The portal ID (index) and Districtr ID (`districtr_id`) are fused into the `Portal Link (Districtr)` column; the raw portal ID is replaced with link to the appropriate mapping portal if a portal URL prefix is available.\n",
    "* `submission_title` and `submission_text` are fused into the `Overall Submission Information` column. The title is **bolded** with `\\textbf{}`.\n",
    "* `area_name` and `area_text` are formatted similarly and fused into the `Individual Area Information` column. `\n",
    "* Labels, which are for internal use only, are removed.\n",
    "\n",
    "We use Pandas' `.to_latex()` to generate an initial $\\LaTeX$ table from this fused DataFrame; all columns are truncated at 2000 characters (approximately 7.143 tweets). We then apply some styling modifications:\n",
    "* We use `supertabular` instead of `tabular` to enable stretching table entries across pages.\n",
    "* We [use `arraystretch` to increase the table's vertical padding](https://tex.stackexchange.com/a/31704).\n",
    "* We fix column widths:\n",
    "  * `Portal Link Districtr` – .48in\n",
    "  * `Overall Submission Information` - 3.5in\n",
    "  *`Individual Area Information` - 2in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_tex_joint_columns(row, columns=('area_name', 'area_text')):\n",
    "  \"\"\"Fuses two columns (name + text) into a single column with conditional formatting.\"\"\"\n",
    "  name = row[columns[0]].strip()\n",
    "  text = row[columns[1]].strip()\n",
    "  if name and text:\n",
    "    return '\\\\textbf{' + name + ':} ' + text\n",
    "  elif name and not text:\n",
    "    return '\\\\textbf{' + name + '.}'\n",
    "  return text\n",
    "\n",
    "\n",
    "def format_tex(submissions_df, portal_url_prefix=None, max_colwidth_chars=2000):\n",
    "  \"\"\"Generates LaTeX submission tables according to MGGG report specs.\"\"\"\n",
    "  submissions_tex = submissions_df.copy()\n",
    "  submissions_tex['portal_id'] = submissions_tex.index.str.split('-').str[0]\n",
    "  submissions_tex['part_id'] = submissions_tex.index.str.split('-').str[1]\n",
    "  submissions_tex['plan_link'] = submissions_tex['portal_id']\n",
    "  submissions_tex = submissions_tex.set_index(['portal_id', 'part_id']).sort_index(level=1).reset_index()\n",
    "  \n",
    "  if portal_url_prefix:\n",
    "    submissions_tex['plan_link'] = submissions_tex['plan_link'].apply(\n",
    "      lambda portal_id: '\\href{' + portal_url_prefix + portal_id.split('-')[0] + '}{' + portal_id + '}'\n",
    "    )\n",
    "  submissions_tex['districtr_id'] = submissions_tex['districtr_id'].str.split('-').str[0].str.strip()\n",
    "  submissions_tex['Portal Link (Districtr)'] = submissions_tex['plan_link'] + ' (' + submissions_tex['districtr_id'] + ')'\n",
    "  submissions_tex['Individual Area Information'] = submissions_tex.apply(format_tex_joint_columns, axis=1)\n",
    "  submissions_tex['Overall Submission Information'] = submissions_tex.apply(\n",
    "    lambda row: format_tex_joint_columns(row, ('submission_title', 'submission_text')),\n",
    "    axis=1\n",
    "  )\n",
    "  display_cols = ['Portal Link (Districtr)', 'Overall Submission Information', 'Individual Area Information']\n",
    "  # oof. (see https://stackoverflow.com/a/46974532)\n",
    "  submissions_tex = submissions_tex[display_cols].set_index(display_cols)\n",
    "  with pd.option_context('max_colwidth', max_colwidth_chars):\n",
    "    tex = submissions_tex.to_latex(index=True, multirow=True)\n",
    "  subs = {\n",
    "    '\\\\textbackslash href\\{': '\\href{',\n",
    "    '\\}\\{': '}{',\n",
    "    '\\} (': '} (',\n",
    "    '\\\\textbackslash n': ' ',  # suppress newlines\n",
    "    '\\\\textbackslash textbf\\{': '\\\\textbf{',\n",
    "    ':\\}': ':}',\n",
    "    '.\\}': '.}'\n",
    "  }\n",
    "  for start, end in subs.items():\n",
    "    tex = tex.replace(start, end)\n",
    "  tex_lines = tex.split('\\n')    \n",
    "  return '\\n'.join(tex_lines[5:-3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_idx, counts in tqdm(enumerate(cluster_counts)):\n",
    "  cluster_id = f'{cluster_name_prefix}{cluster_idx + 1}'\n",
    "  cluster_label = f'{output_prefix}_cluster_{cluster_id}'\n",
    "\n",
    "  df = pd.DataFrame.from_dict(counts, orient='index', columns=['count'])\n",
    "  df.index.name = 'GEOID20'\n",
    "  df['freq'] = df['count'] / df['count'].max()\n",
    "  submissions = clusters[clusters['cluster'] == cluster_id][filtered_output_columns]\n",
    "  #for col in ('submission_text', 'area_name', 'area_text'):\n",
    "  #  submissions[col] = submissions[col].str.replace('\\\\n', '\\n').str.replace('\\\\t', ' ')\n",
    "\n",
    "  if 'csv_geo' in output_formats:\n",
    "    df.to_csv(f'{full_output_dir}/csv_geo/{cluster_label}.csv')\n",
    "    \n",
    "  if 'csv_comments' in output_formats:\n",
    "    submissions.to_csv(f'{full_output_dir}/csv_comments/{cluster_label}.csv')\n",
    "    \n",
    "  if 'html' in output_formats:\n",
    "    html = submissions.to_html()\n",
    "    html = html.replace('\\\\n', '<br>').replace('\\\\t', ' ')\n",
    "    with open(f'{full_output_dir}/html/{cluster_label}.html', 'w') as f:\n",
    "      if portal_url_prefix:\n",
    "        for portal_id in submissions.index:\n",
    "          url = portal_url_prefix + portal_id.split('-')[0]\n",
    "          html = html.replace(\n",
    "            portal_id,\n",
    "            f'<a href=\"{url}\" target=\"_blank\">{portal_id}</a>'\n",
    "          )\n",
    "      f.write(html)\n",
    "  \n",
    "  if 'tex' in output_formats:    \n",
    "      with open(f'{full_output_dir}/tex/{cluster_label}.tex', 'w') as f:\n",
    "        f.write(format_tex(submissions, portal_url_prefix))\n",
    "  \n",
    "  if 'shapefile' in output_formats or 'png' in output_formats:\n",
    "    gdf = gpd.GeoDataFrame(df).join(blocks_2020_gdf[['geometry']])\n",
    "    gdf.crs = crs\n",
    "    if force_connected:\n",
    "      # Fetching block-level 2020 populations is still annoying as of\n",
    "      # 2021-08-25---the Census API still only has 2000/2010 data---\n",
    "      # so we use block count as a rough proxy for population-weighted\n",
    "      # component size.\n",
    "      subgraph = nx.subgraph(graph, list(gdf.index))\n",
    "      components = nx.connected_components(subgraph)\n",
    "      largest_component = sorted(components, key=len)[-1]\n",
    "      component_ids = set(graph.nodes[b]['GEOID20'] for b in largest_component)\n",
    "      gdf = gdf[gdf.index.to_series().isin(component_ids)]\n",
    "    \n",
    "    if 'shapefile' in output_formats:\n",
    "      gdf.to_file(f'{full_output_dir}/shapefile/{cluster_label}')\n",
    "      \n",
    "    if 'png' in output_formats:\n",
    "      blocks_2020_gdf['count'] = gdf['count']\n",
    "      blocks_2020_gdf['count'] = blocks_2020_gdf['count'].fillna(0)\n",
    "      fig, ax = plt.subplots(figsize=(10, 8), dpi=100)\n",
    "      counties_gdf.plot(ax=ax, edgecolor='black', linewidth=2)\n",
    "      counties_gdf.plot(color='#fffff5', edgecolor='#e5e5e5', ax=ax)\n",
    "      gdf.plot(ax=ax, column='count', cmap='YlOrRd', #'viridis_r',\n",
    "                           linewidth=0, edgecolor='none', antialiased=False,\n",
    "                           vmin=0, vmax=10)\n",
    "      ax.axis('off')  \n",
    "      plt.savefig(f'{full_output_dir}/png/{cluster_id}.png',\n",
    "                  dpi=300, transparent=True, bbox_inches='tight')\n",
    "      plt.close()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "avg_hausdorff_clustering_for_pipeline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
