{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1711f44-f1ce-478f-ab00-9f9fb4e641b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a08e4b-022f-457a-9f2c-28684ad91f3f",
   "metadata": {},
   "source": [
    "## Per-cluster geo+semantic pipeline\n",
    "\n",
    "Moon suggests:\n",
    "* Leave clusters that are roughly the desired size alone.\n",
    "* For clusters that are too large, run JN's Markov chain to the split the clusters into $k$ parts. _(Question: do we fix $k$ for all clusters? Should $k$ vary linearly with the number of submissions in a cluster? Is this once again an ad-hoc thing?)_\n",
    "\n",
    "We will need to recombine the split clusters with the untouched clusters to create a final data product. The flow for the `A` clusters (geographical clustering only) is `preprocess_submissions` → `generate_geo_clusters` → `cluster_outputs`, where `generate_geo_clusters` is a minimal wrapper around `ccdb`. A reasonable flow for this analysis might be:\n",
    "* `preprocess_submissions` (downloads data and joins labels—same as before)\n",
    "* `generate_geo_clusters` (call geographical clustering algorithm—same as before)\n",
    "* this notebook (generates matrices from specified superclusters)\n",
    "for each cluster to split:\n",
    "  * JN's notebook (generates labelings from the matrices)\n",
    "  * another new notebook (generates a CSV or pickle file usable by `cluster_outputs` as is)\n",
    "\n",
    "We save the matrices in `outputs`, as they are intermediate files used only by this pipeline and therefore not worth cataloguing in `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f883f4-7de9-4488-a7e0-d5ba20d07d08",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "db_path = '../../MI/data/mi_cluster_db_20210825.pkl'\n",
    "output_dir = '../../MI/outputs'\n",
    "output_prefix = 'mi_cluster_db_20210823'\n",
    "num_clusters = 36\n",
    "clusters_to_split = (22, 32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f854eb9-80a7-47ed-906c-961d7894c35c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "db = pickle.load(open(db_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaef312a-4f46-46d8-937c-a73373b52653",
   "metadata": {},
   "outputs": [],
   "source": [
    "db.coi_data['idx'] = range(len(db.coi_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa57def-4ed7-4461-bca7-22d8a154e62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df = db.clusters_from_number(num_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a91fa0-4718-4cb3-9cae-6418873c8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusters_df['labels'] = clusters_df['labels'].apply(literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f5abc9-1040-46cc-9d70-d59367603f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard_similarity_matrix(cluster_df):\n",
    "  \"\"\"Generates a Jaccard similarity matrix over unique labels.\"\"\"\n",
    "  unique_labels = {\n",
    "    label: idx\n",
    "    for idx, label in enumerate(set.union(*(set(labels) for labels in cluster_df['labels'])))\n",
    "  }\n",
    "  \n",
    "  n = len(cluster_df)\n",
    "  label_vectors = np.zeros((n, len(unique_labels)), dtype=int)\n",
    "  for idx, labels in enumerate(cluster_df['labels']):\n",
    "    for label in labels:\n",
    "      if label in unique_labels:\n",
    "        label_vectors[idx, unique_labels[label]] = 1\n",
    "        \n",
    "  semantic_similarities = np.zeros((n, n))\n",
    "  for ii, outer_vec in enumerate(label_vectors):\n",
    "    for jj, inner_vec in enumerate(label_vectors):\n",
    "      inter = np.bitwise_and(inner_vec, outer_vec)\n",
    "      union = np.bitwise_or(inner_vec, outer_vec)\n",
    "      semantic_similarities[ii, jj] = inter.sum() / max(union.sum(), 1)\n",
    "  return semantic_similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e8497d-fd4f-47c1-80b1-b622ccc9f851",
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_id in clusters_to_split:\n",
    "  cluster_df = clusters_df[clusters_df['clusters'] == int(cluster_id)]\n",
    "  jaccard_sims = jaccard_similarity_matrix(cluster_df)\n",
    "  np.savetxt(\n",
    "    os.path.join(output_dir, f'{output_prefix}_cluster_{cluster_id}_jaccard_sims.txt'),\n",
    "    jaccard_sims\n",
    "  )\n",
    "    \n",
    "  indices = cluster_df['idx']\n",
    "  hausdorff_distances = db.coi_total_dissimilarities[indices][:, indices]\n",
    "  np.savetxt(\n",
    "    os.path.join(output_dir, f'{output_prefix}_cluster_{cluster_id}_hausdorff_dists.txt'),\n",
    "    hausdorff_distances\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56975946-bbaa-4d29-8481-9258dcd03b83",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
